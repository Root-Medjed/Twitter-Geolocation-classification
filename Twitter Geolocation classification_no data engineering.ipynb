{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.Read the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#define a function to import csv\n",
    "\n",
    "def import_csv(csvFile): #argument - pass a string\n",
    "    df = pd.read_csv(csvFile)\n",
    "    return df\n",
    "\n",
    "#import 12 datasets provided\n",
    "train_full_raw = import_csv(\"train_full.csv\") \n",
    "train_count_raw = import_csv(\"train_count.csv\")\n",
    "train_tfidf_raw = import_csv(\"train_tfidf.csv\")\n",
    "\n",
    "\n",
    "dev_full_raw = import_csv(\"dev_full.csv\")\n",
    "dev_count_raw = import_csv(\"dev_count.csv\")\n",
    "dev_tfidf_raw = import_csv(\"dev_tfidf.csv\")\n",
    "\n",
    "\n",
    "test_full_raw = import_csv(\"test_full.csv\")\n",
    "test_count_raw = import_csv(\"test_count.csv\")\n",
    "test_tfidf_raw = import_csv(\"test_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extract Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and dev labels - \"region\" column\n",
    "train_labels = train_full_raw['region']\n",
    "dev_labels = dev_full_raw['region']\n",
    "\n",
    "#data features - \"tweet\" column\n",
    "train_full = train_full_raw['tweet']\n",
    "train_count = train_count_raw['tweet']\n",
    "train_tfidf = train_tfidf_raw['tweet']\n",
    "\n",
    "\n",
    "dev_full = dev_full_raw['tweet']\n",
    "dev_count = dev_count_raw['tweet']\n",
    "dev_tfidf = dev_tfidf_raw['tweet']\n",
    "\n",
    "\n",
    "test_full = test_full_raw['tweet']\n",
    "test_count = test_count_raw['tweet']\n",
    "test_tfidf = test_tfidf_raw['tweet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 read \"vocab.txt\" File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \"vocab.tex\" to build a vocab dictionary\n",
    "#purpose: to be able to trace back for the original 'word' using 'word code'\n",
    "\n",
    "vocab_dict = {}\n",
    "vocab = open(\"vocab.txt\", encoding=\"utf8\") #\"r\" - read line by line \n",
    "vocab_lines = vocab.readlines()\n",
    "\n",
    "for line in vocab_lines:\n",
    "    value, key = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "    vocab_dict[int(key)] = value\n",
    "#print(vocab_dict)\n",
    "#print(len(vocab_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 build vector for each of the imported dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def vector_transform (dataset): \n",
    "    result = []\n",
    "    for line in dataset:\n",
    "        line_cleaned = re.findall(r'[(](.*?)[)]', line) # ['1111, 1.0', '1875, 1.0']\n",
    "        aux_line = [0 for i in range(len(vocab_dict))] #build an list of '0's with lenth of 2038\n",
    "        for elm in line_cleaned:\n",
    "            inx, value = elm.split(\",\")\n",
    "            aux_line[int(inx)] = float(value)\n",
    "        result.append(aux_line)\n",
    "    return result\n",
    "\n",
    "def vector_transform_glove300(glove300):\n",
    "    output = []\n",
    "    for item in glove300:\n",
    "        lst = item.split(\" \")\n",
    "        aux_line = []\n",
    "        for elm in lst:\n",
    "            aux_line.append(float(elm))\n",
    "        output.append(aux_line)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "train_count_vector = vector_transform(train_count) \n",
    "train_tfidf_vector = vector_transform(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev\n",
    "dev_count_vector = vector_transform(dev_count)\n",
    "dev_tfidf_vector = vector_transform(dev_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "test_count_vector = vector_transform(test_count)\n",
    "test_tfidf_vector = vector_transform(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataframes\n",
    "df_tfidf = pd.DataFrame(train_tfidf_vector)\n",
    "df_count = pd.DataFrame(train_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2028</th>\n",
       "      <th>2029</th>\n",
       "      <th>2030</th>\n",
       "      <th>2031</th>\n",
       "      <th>2032</th>\n",
       "      <th>2033</th>\n",
       "      <th>2034</th>\n",
       "      <th>2035</th>\n",
       "      <th>2036</th>\n",
       "      <th>2037</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11470</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11471</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11472</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11473</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11474</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11475 rows × 2038 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9     ...  2028  \\\n",
       "0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "11470   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "11471   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "11472   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "11473   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "11474   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "       2029  2030  2031  2032  2033  2034  2035  2036  2037  \n",
       "0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "11470   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "11471   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "11472   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "11473   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "11474   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[11475 rows x 2038 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev dataframes\n",
    "dev_tfidf = pd.DataFrame(dev_tfidf_vector)\n",
    "dev_tfidf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Zero-R Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The marority class is: NORTHEAST\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "majority_class = []\n",
    "zero_r_predictions = []\n",
    "\n",
    "t_labels = train_labels.values.tolist()\n",
    "\n",
    "majority_class = max(set(t_labels), key=t_labels.count)\n",
    "\n",
    "zero_r_predictions = [majority_class for i in range(len(dev_labels))]\n",
    "\n",
    "print(f\"The marority class is: {majority_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Naive Bayes Classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def nb_Gaussian (x_train, y_train):\n",
    "    \n",
    "    gb = GaussianNB()\n",
    "    gb.fit(x_train, y_train)\n",
    "    \n",
    "    return gb\n",
    "\n",
    "def nb_Bernoulli (x_train, y_train):\n",
    "    \n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(x_train, y_train)\n",
    "    \n",
    "    return bnb\n",
    "\n",
    "def nb_multinomial (x_train, y_train):\n",
    "    \n",
    "    mb = MultinomialNB()\n",
    "    mb.fit(x_train, y_train)\n",
    "    \n",
    "    return mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implementing the Naive Bayes Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GaussianNB - build models\n",
    "gb_model_count = nb_Gaussian(train_count_vector, train_labels)\n",
    "gb_model_tfidf = nb_Gaussian(train_tfidf_vector, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions - GaussianNB\n",
    "predictions_count_dev_gb = gb_model_count.predict(dev_count_vector)\n",
    "predictions_tfidf_dev_gb = gb_model_tfidf.predict(dev_tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BernoulliNB - build models\n",
    "bnb_model_count = nb_Bernoulli(train_count_vector, train_labels)\n",
    "bnb_model_tfidf = nb_Bernoulli(train_tfidf_vector, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions - BernoulliNB\n",
    "predictions_count_dev_bnb = bnb_model_count.predict(dev_count_vector)\n",
    "predictions_tfidf_dev_bnb = bnb_model_tfidf.predict(dev_tfidf_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultinomialNB - build models\n",
    "mb_model_count = nb_multinomial(train_count_vector, train_labels)\n",
    "mb_model_tfidf = nb_multinomial(train_tfidf_vector, train_labels) \n",
    "#cannot apply to glove300 as it contains negative numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions - MultinomialNB\n",
    "predictions_count_dev_mb = mb_model_count.predict(dev_count_vector)\n",
    "predictions_tfidf_dev_mb = mb_model_tfidf.predict(dev_tfidf_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     MIDWEST       0.21      0.03      0.06      1484\n",
      "   NORTHEAST       0.51      0.61      0.56      4295\n",
      "       SOUTH       0.43      0.58      0.49      4266\n",
      "        WEST       0.18      0.05      0.08      1430\n",
      "\n",
      "    accuracy                           0.45     11475\n",
      "   macro avg       0.33      0.32      0.30     11475\n",
      "weighted avg       0.40      0.45      0.41     11475\n",
      "\n",
      "[[  47  475  917   45]\n",
      " [  62 2621 1518   94]\n",
      " [  82 1497 2466  221]\n",
      " [  32  509  812   77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     MIDWEST       0.00      0.00      0.00      1484\n",
      "   NORTHEAST       0.50      0.62      0.56      4295\n",
      "       SOUTH       0.43      0.62      0.51      4266\n",
      "        WEST       0.28      0.01      0.01      1430\n",
      "\n",
      "    accuracy                           0.46     11475\n",
      "   macro avg       0.30      0.31      0.27     11475\n",
      "weighted avg       0.38      0.46      0.40     11475\n",
      "\n",
      "[[   0  496  987    1]\n",
      " [   0 2683 1612    0]\n",
      " [   0 1617 2627   22]\n",
      " [   1  574  846    9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "#confusion matrix mnb count\n",
    "print(metrics.classification_report(dev_labels, predictions_count_dev_mb))\n",
    "print(metrics.confusion_matrix(dev_labels, predictions_count_dev_mb))\n",
    "#confusion matrix mnb tfidf\n",
    "print(metrics.classification_report(dev_labels,predictions_tfidf_dev_mb))\n",
    "print(metrics.confusion_matrix(dev_labels,predictions_tfidf_dev_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     MIDWEST       0.21      0.03      0.05      1484\n",
      "   NORTHEAST       0.51      0.62      0.56      4295\n",
      "       SOUTH       0.43      0.58      0.50      4266\n",
      "        WEST       0.19      0.05      0.07      1430\n",
      "\n",
      "    accuracy                           0.46     11475\n",
      "   macro avg       0.33      0.32      0.29     11475\n",
      "weighted avg       0.40      0.46      0.41     11475\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     MIDWEST       0.21      0.03      0.05      1484\n",
      "   NORTHEAST       0.51      0.62      0.56      4295\n",
      "       SOUTH       0.43      0.58      0.50      4266\n",
      "        WEST       0.19      0.05      0.07      1430\n",
      "\n",
      "    accuracy                           0.46     11475\n",
      "   macro avg       0.33      0.32      0.29     11475\n",
      "weighted avg       0.40      0.46      0.41     11475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix bnb count\n",
    "print(metrics.classification_report(dev_labels, predictions_count_dev_bnb))\n",
    "\n",
    "#confusion matrix bnb tfidf\n",
    "print(metrics.classification_report(dev_labels,predictions_tfidf_dev_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def logisticR (x_train, y_train):\n",
    "    \n",
    "    lr = LogisticRegression(penalty='l2', max_iter = 300, multi_class = 'multinomial')\n",
    "    lr.fit(x_train, y_train)\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tuohuang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#build model - count\n",
    "lr_count_model = logisticR (train_count_vector, train_labels)\n",
    "\n",
    "#make predictions - count\n",
    "predictions_count_dev_lr = lr_count_model.predict(dev_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tuohuang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#build model - tfidf\n",
    "lr_tfidf_model = logisticR (train_count_vector, train_labels)\n",
    "#make predictions - tfidf\n",
    "predictions_tfidf_dev_lr = lr_tfidf_model.predict(dev_tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     MIDWEST       0.24      0.02      0.03      1484\n",
      "   NORTHEAST       0.50      0.63      0.56      4295\n",
      "       SOUTH       0.43      0.58      0.50      4266\n",
      "        WEST       0.20      0.03      0.05      1430\n",
      "\n",
      "    accuracy                           0.46     11475\n",
      "   macro avg       0.34      0.31      0.28     11475\n",
      "weighted avg       0.40      0.46      0.40     11475\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     MIDWEST       0.00      0.00      0.00      1484\n",
      "   NORTHEAST       0.45      0.73      0.56      4295\n",
      "       SOUTH       0.44      0.45      0.44      4266\n",
      "        WEST       0.56      0.00      0.01      1430\n",
      "\n",
      "    accuracy                           0.44     11475\n",
      "   macro avg       0.36      0.30      0.25     11475\n",
      "weighted avg       0.40      0.44      0.37     11475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix LR count\n",
    "print(metrics.classification_report(dev_labels, predictions_count_dev_lr))\n",
    "\n",
    "#confusion matrix LR tfidf\n",
    "print(metrics.classification_report(dev_labels,predictions_tfidf_dev_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Linear SVC\n",
    "def lin_svm_clf (x_train, y_train):\n",
    "    lin_clf = svm.LinearSVC()\n",
    "    lin_clf.fit(x_train, y_train)\n",
    "  \n",
    "    return lin_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tuohuang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "#build models - count\n",
    "svm_count_model = lin_svm_clf(train_count_vector, train_labels)\n",
    "#make predictions -count\n",
    "predictions_count_dev_svm = svm_count_model.predict(dev_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build models - tfidf\n",
    "svm_tfidf_model = lin_svm_clf(train_tfidf_vector, train_labels)\n",
    "#make predictions - tfidf\n",
    "predictions_tfidf_dev_svm = svm_tfidf_model.predict(dev_tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     MIDWEST       0.24      0.00      0.01      1484\n",
      "   NORTHEAST       0.49      0.65      0.56      4295\n",
      "       SOUTH       0.44      0.58      0.50      4266\n",
      "        WEST       0.22      0.03      0.05      1430\n",
      "\n",
      "    accuracy                           0.46     11475\n",
      "   macro avg       0.35      0.31      0.28     11475\n",
      "weighted avg       0.40      0.46      0.40     11475\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     MIDWEST       0.18      0.00      0.00      1484\n",
      "   NORTHEAST       0.50      0.63      0.55      4295\n",
      "       SOUTH       0.43      0.60      0.50      4266\n",
      "        WEST       0.26      0.03      0.05      1430\n",
      "\n",
      "    accuracy                           0.46     11475\n",
      "   macro avg       0.34      0.31      0.28     11475\n",
      "weighted avg       0.40      0.46      0.40     11475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix LR count\n",
    "print(metrics.classification_report(dev_labels, predictions_count_dev_svm))\n",
    "\n",
    "#confusion matrix LR tfidf\n",
    "print(metrics.classification_report(dev_labels,predictions_tfidf_dev_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def SGD_svm (x_train, y_train):\n",
    "    \n",
    "    stochastic_SVM = SGDClassifier(loss='hinge', shuffle=True, random_state=101, alpha=0.0001, l1_ratio=0.15)\n",
    "    stochastic_SVM.fit(x_train, y_train)\n",
    "  \n",
    "    return stochastic_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC COUNT\n",
    "svm_count_SGD = SGD_svm(train_count_vector, train_labels)\n",
    "predictions_count_dev_SGD = svm_count_SGD.predict(dev_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC TF-IDF\n",
    "svm_tfidf_SGD = SGD_svm(train_tfidf_vector, train_labels)\n",
    "predictions_tfidf_dev_SGD = svm_tfidf_SGD.predict(dev_tfidf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def accurate(labels_observed, labels_predicted):\n",
    "    accuracy = 0\n",
    "    accuracy = metrics.accuracy_score(labels_observed, labels_predicted)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero R\t\t\t\t\t\tAccuracy: 0.3743\tMacro F1: 0.1362\n",
      "GaussianNB apply on 'count'\t\t\tAccuracy: 0.2249\tMacro F1: 0.212\n",
      "GaussianNB apply on 'tfidf'\t\t\tAccuracy: 0.2309\tMacro F1: 0.219\n",
      "BernoulliNB apply on 'count'\t\t\tAccuracy: 0.4565\tMacro F1: 0.2949\n",
      "BernoulliNB apply on 'tfidf'\t\t\tAccuracy: 0.4565\tMacro F1: 0.2949\n",
      "MultinomialNB apply on 'count'\t\t\tAccuracy: 0.4541\tMacro F1: 0.2974\n",
      "MultinomialNB apply on 'tfidf'\t\t\tAccuracy: 0.4635\tMacro F1: 0.2689\n",
      "Logistic Regression apply on 'count'\t\tAccuracy: 0.4577\tMacro F1: 0.2837\n",
      "Logistic Regression apply on 'tfidf'\t\tAccuracy: 0.4431\tMacro F1: 0.2517\n",
      "SVM LINEAR apply on 'count'\t\t\tAccuracy: 0.4593\tMacro F1: 0.2772\n",
      "SVM LINEAR apply on 'tfidf'\t\t\tAccuracy: 0.4609\tMacro F1: 0.2765\n",
      "SVM SGD apply on 'count'\t\t\tAccuracy: 0.4505\tMacro F1: 0.278\n",
      "SVM SGD apply on 'tfidf'\t\t\tAccuracy: 0.433\t\tMacro F1: 0.273\n"
     ]
    }
   ],
   "source": [
    "#Zero-R\n",
    "zero_r_acc = accurate(dev_labels, zero_r_predictions)\n",
    "zero_r_f1 = f1_score(dev_labels, zero_r_predictions, average='macro')\n",
    "\n",
    "#GaussianNB on COUNT\n",
    "gb_count_acc = accurate(dev_labels, predictions_count_dev_gb)\n",
    "gb_count_f1 = f1_score(dev_labels, predictions_count_dev_gb, average='macro')\n",
    "\n",
    "#BernoulliNB on COUNT\n",
    "bnb_count_acc = accurate(dev_labels, predictions_count_dev_bnb)\n",
    "bnb_count_f1 = f1_score(dev_labels, predictions_count_dev_bnb, average='macro')\n",
    "\n",
    "#MultinomialNB on count\n",
    "mb_count_acc = accurate(dev_labels, predictions_count_dev_mb)\n",
    "mb_count_f1 = f1_score(dev_labels, predictions_count_dev_mb, average='macro')\n",
    "\n",
    "#GaussianNB on TFIDF\n",
    "gb_tfidf_acc = accurate(dev_labels, predictions_tfidf_dev_gb)\n",
    "gb_tfidf_f1 = f1_score(dev_labels, predictions_tfidf_dev_gb, average='macro')\n",
    "\n",
    "#BernoulliNB on TFIDF\n",
    "bnb_tfidf_acc = accurate(dev_labels, predictions_tfidf_dev_bnb)\n",
    "bnb_tfidf_f1 = f1_score(dev_labels, predictions_tfidf_dev_bnb, average='macro')\n",
    "\n",
    "#MultinomialNB on TFIDF\n",
    "mb_tfidf_acc = accurate(dev_labels, predictions_tfidf_dev_mb)\n",
    "mb_tfidf_f1 = f1_score(dev_labels, predictions_tfidf_dev_mb, average='macro')\n",
    "\n",
    "#Logistic Regression on COUNT\n",
    "lr_count_acc = accurate(dev_labels, predictions_count_dev_lr)\n",
    "lr_count_f1 = f1_score(dev_labels, predictions_count_dev_lr, average='macro')\n",
    "\n",
    "#Logistic Regression on TFIDF\n",
    "lr_tfidf_acc = accurate(dev_labels, predictions_tfidf_dev_lr)\n",
    "lr_tfidf_f1 = f1_score(dev_labels, predictions_tfidf_dev_lr, average='macro')\n",
    "\n",
    "#SVM LINEAR on COUNT\n",
    "svm_count_acc = accurate(dev_labels, predictions_count_dev_svm)\n",
    "svm_count_f1 = f1_score(dev_labels, predictions_count_dev_svm, average='macro')\n",
    "\n",
    "#SVM LINEAR on TFIDF\n",
    "svm_tfidf_acc = accurate(dev_labels, predictions_tfidf_dev_svm)\n",
    "svm_tfidf_f1 = f1_score(dev_labels, predictions_tfidf_dev_svm, average='macro')\n",
    "\n",
    "#SVM SGD on COUNT\n",
    "svm_count_SGD_acc = accurate(dev_labels, predictions_count_dev_SGD)\n",
    "svm_count_SGD_f1 = f1_score(dev_labels, predictions_count_dev_SGD, average='macro')\n",
    "\n",
    "#SVM SGD on TFIDF\n",
    "svm_tfidf_SGD_acc = accurate(dev_labels, predictions_tfidf_dev_SGD)\n",
    "svm_tfidf_SGD_f1 = f1_score(dev_labels, predictions_tfidf_dev_SGD, average='macro')\n",
    "\n",
    "print(f\"Zero R\\t\\t\\t\\t\\t\\tAccuracy: {round(zero_r_acc, 4)}\\tMacro F1: {round(zero_r_f1, 4)}\")\n",
    "print(f\"GaussianNB apply on 'count'\\t\\t\\tAccuracy: {round(gb_count_acc, 4)}\\tMacro F1: {round(gb_count_f1, 4)}\")\n",
    "print(f\"GaussianNB apply on 'tfidf'\\t\\t\\tAccuracy: {round(gb_tfidf_acc, 4)}\\tMacro F1: {round(gb_tfidf_f1, 4)}\")\n",
    "\n",
    "print(f\"BernoulliNB apply on 'count'\\t\\t\\tAccuracy: {round(bnb_count_acc, 4)}\\tMacro F1: {round(bnb_count_f1, 4)}\")\n",
    "print(f\"BernoulliNB apply on 'tfidf'\\t\\t\\tAccuracy: {round(bnb_tfidf_acc, 4)}\\tMacro F1: {round(bnb_tfidf_f1, 4)}\")\n",
    "\n",
    "print(f\"MultinomialNB apply on 'count'\\t\\t\\tAccuracy: {round(mb_count_acc, 4)}\\tMacro F1: {round (mb_count_f1, 4)}\")\n",
    "print(f\"MultinomialNB apply on 'tfidf'\\t\\t\\tAccuracy: {round(mb_tfidf_acc, 4)}\\tMacro F1: {round(mb_tfidf_f1, 4)}\")\n",
    "\n",
    "print(f\"Logistic Regression apply on 'count'\\t\\tAccuracy: {round(lr_count_acc, 4)}\\tMacro F1: {round(lr_count_f1, 4)}\")\n",
    "print(f\"Logistic Regression apply on 'tfidf'\\t\\tAccuracy: {round(lr_tfidf_acc, 4)}\\tMacro F1: {round(lr_tfidf_f1, 4)}\")\n",
    "\n",
    "print(f\"SVM LINEAR apply on 'count'\\t\\t\\tAccuracy: {round(svm_count_acc, 4)}\\tMacro F1: {round(svm_count_f1, 4)}\")\n",
    "print(f\"SVM LINEAR apply on 'tfidf'\\t\\t\\tAccuracy: {round(svm_tfidf_acc, 4)}\\tMacro F1: {round(svm_tfidf_f1, 4)}\")\n",
    "\n",
    "print(f\"SVM SGD apply on 'count'\\t\\t\\tAccuracy: {round(svm_count_SGD_acc, 4)}\\tMacro F1: {round(svm_count_SGD_f1, 4)}\")\n",
    "print(f\"SVM SGD apply on 'tfidf'\\t\\t\\tAccuracy: {round(svm_tfidf_SGD_acc, 4)}\\t\\tMacro F1: {round(svm_tfidf_SGD_f1, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20) (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# synthetic classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=5, n_redundant=15, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 20) (3000, 20) (7000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "# split a dataset into train and test sets\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=5, n_redundant=15, random_state=1)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# summarize the shape of the train and test sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
